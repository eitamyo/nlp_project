{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c72b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistralai\n",
      "  Downloading mistralai-1.9.10-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
      "  Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (2.11.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (6.0.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (0.4.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
      "Downloading mistralai-1.9.10-py3-none-any.whl (440 kB)\n",
      "Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: invoke, eval-type-backport, mistralai\n",
      "\n",
      "   ---------------------------------------- 0/3 [invoke]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   -------------------------- ------------- 2/3 [mistralai]\n",
      "   ---------------------------------------- 3/3 [mistralai]\n",
      "\n",
      "Successfully installed eval-type-backport-0.2.2 invoke-2.2.0 mistralai-1.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f387c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e2bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f09edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BaseModelCard(id='mistral-medium-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-medium-2505', description='Our frontier-class multimodal model released May 2025.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-large-latest', description='Official mistral-large-latest Mistral AI model', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-latest', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='ministral-3b-2410', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758794124, owned_by='mistralai', name='ministral-3b-2410', description='Official ministral-3b-2410 Mistral AI model', max_context_length=32768, aliases=['ministral-3b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='ministral-3b-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758794124, owned_by='mistralai', name='ministral-3b-2410', description='Official ministral-3b-2410 Mistral AI model', max_context_length=32768, aliases=['ministral-3b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='ministral-8b-2410', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='ministral-8b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-7b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['mistral-tiny', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-2312', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-nemo', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-nemo-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x7b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['mistral-small', 'mistral-small-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['open-mixtral-8x7b', 'mistral-small-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2312', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['open-mixtral-8x7b', 'mistral-small'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x22b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mixtral-8x22b', description='Our best open source model to date released April 2024. ', max_context_length=65536, aliases=['open-mixtral-8x22b-2404'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x22b-2404', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='open-mixtral-8x22b', description='Our best open source model to date released April 2024. ', max_context_length=65536, aliases=['open-mixtral-8x22b'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-small-2409', description='Our latest enterprise-grade small model with the latest version v2 released September 2024. ', max_context_length=32768, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-large-2407', description='Our top-tier reasoning model for high-complexity tasks with the latest version v2 released July 2024.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-large-2411', description='Our top-tier reasoning model for high-complexity tasks with the lastest version released November 2024.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-latest', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-pixtral-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'pixtral-large-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2412', 'codestral-2411-rc5'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2412', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2501', 'codestral-2411-rc5'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2411-rc5', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2501', 'codestral-2412'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-2508'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='devstral-small-2505', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-small-2501', description='Our latest enterprise-grade small model with the latest version released January 2025. ', max_context_length=32768, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2503', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-small-2503', description='Our latest enterprise-grade small model with the latest version released March 2025.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-2506'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-saba-2502', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-saba-2502', description='Official mistral-saba-2502 Mistral AI model', max_context_length=32768, aliases=['mistral-saba-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-saba-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-saba-2502', description='Official mistral-saba-2502 Mistral AI model', max_context_length=32768, aliases=['mistral-saba-2502'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-medium-2506', description='Our frontier-class reasoning model released June 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-medium-2507', description='Our frontier-class reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-small-2506', description='Our efficient reasoning model released June 2025.', max_context_length=40000, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-small-2507', description='Our efficient reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='mistral-embed-2312', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='codestral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='codestral-embed-2505', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-moderation-2411', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758794124, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-moderation-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758794124, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-2503', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-ocr-2503', description='Official mistral-ocr-2503 Mistral AI model', max_context_length=16384, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758794124, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-transcribe-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758794124, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "client.models.list().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b75c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "nikud_prompt = \"\"\"\n",
    "Add full Hebrew nikud (vowel marks) to the following text. \n",
    "Return only the hebrew text with nikud. Do not provide explanations or notes, or any other text other than the hebrew one. \n",
    "Text: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7dc0c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['מְמַלְכַּת רוֹהַן הָיְתָה מְמַלְכָּה בְּדִיּוֹנִית שֶׁנּוֹצְרָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעוֹת\".', 'מְמַלְכַּת רוֹהָן הָיְתָה מְמַלְכָּה בְּדִיּוֹנִית שֶׁנִּצְרְתָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטֶּבַע\".', 'מְמַלְכַּת רוֹהָן הָיְתָה מְמַלְכָּה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטֶּבַע\".', 'מְמַלְכַּת רוֹהָן הָיְתָה מְמַלְכָּה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעָה\".', 'מַמְלָכַת רוֹהַן הָיְתָה מַמְלָכָה בְּדִיּוֹנִית שֶׁנִּצְרְתָה עַל־יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעוֹת\".']\n"
     ]
    }
   ],
   "source": [
    "# model = \"codestral-2501\"\n",
    "model = \"mistral-small-latest\"\n",
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    n=5,\n",
    "    temperature=0.6,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": nikud_prompt + \"ממלכת רוהן הייתה ממלכה בדיונית שנוצרה על ידי ג'.ר.ר. טולקין בספרו \\\"שר הטבעות\\\".\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print([choice.message.content for choice in chat_response.choices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdfda9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv('./datasets/hewiki/hebrew_nikud_dataset_filtered_word_mask.csv')\n",
    "hewiki_sample = filtered_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "936d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_word_entropy_mask(original_sentence: str, outputs: list[str], threshold: float = 0.0):\n",
    "    \"\"\"\n",
    "    Compute entropy-based ambiguity mask for words in a sentence.\n",
    "    \n",
    "    Args:\n",
    "        original_sentence (str): input sentence without nikud (used for tokenization).\n",
    "        outputs (list[str]): list of generated outputs with nikud (multiple generations).\n",
    "        threshold (float): entropy threshold above which a word is marked ambiguous.\n",
    "                           Default = 0.0 (any disagreement → ambiguous).\n",
    "    \n",
    "    Returns:\n",
    "        mask (list[int]): 0/1 mask, length = number of words in original_sentence.\n",
    "                          1 = ambiguous (uncertain), 0 = consistent (confident).\n",
    "        entropies (list[float]): entropy per word (in bits).\n",
    "    \"\"\"\n",
    "    # Split original text into words (no nikud, for alignment)\n",
    "    words = original_sentence.split()\n",
    "    n_words = len(words)\n",
    "\n",
    "    # Collect nikud-versions per word across outputs\n",
    "    word_variants = [ [] for _ in range(n_words) ]\n",
    "    for out in outputs:\n",
    "        out_words = out.split()\n",
    "        if len(out_words) != n_words:\n",
    "            # simple alignment fallback: skip misaligned outputs\n",
    "            continue\n",
    "        for i, w in enumerate(out_words):\n",
    "            word_variants[i].append(w)\n",
    "\n",
    "    entropies = []\n",
    "    mask = []\n",
    "    for variants in word_variants:\n",
    "        if not variants:\n",
    "            entropies.append(0.0)\n",
    "            mask.append(0)\n",
    "            continue\n",
    "        counts = Counter(variants)\n",
    "        total = sum(counts.values())\n",
    "        probs = [c/total for c in counts.values()]\n",
    "        entropy = -sum(p * math.log2(p) for p in probs)\n",
    "        entropies.append(entropy)\n",
    "        mask.append(1 if entropy > threshold else 0)\n",
    "\n",
    "    return mask, entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e63c7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hebrew_tokenizer as ht\n",
    "\n",
    "def strip_nikud(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove Hebrew nikud (vowel marks) from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input Hebrew text with nikud.\n",
    "    \"\"\"\n",
    "    return ht.NIKUD_PATTERN.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed32277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def generate_content_for_prompt(client: Mistral, prompt: str, n_outputs: int, model_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    An asynchronous helper function to call the API for a single prompt \n",
    "    and extract the generated text.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Use the asynchronous client module (client.aio)\n",
    "            chat_response = await client.chat.complete_async(\n",
    "                model=model_name,\n",
    "                n=n_outputs,\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # print(f\"Completed request\")\n",
    "\n",
    "            # Extract the text from each candidate\n",
    "            results = [\n",
    "                choice.message.content for choice in chat_response.choices]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                print(\n",
    "                    f\"Quota exceeded {e}. Waiting for 30 seconds before retrying...\")\n",
    "                await asyncio.sleep(30)  # Wait before retrying\n",
    "                continue  # Retry the request\n",
    "            # print(f\"Request failed with error: {e}\")\n",
    "            # Return error message for a failed request\n",
    "            raise RuntimeError(f\"Request failed with error: {e}\")\n",
    "\n",
    "\n",
    "async def run_async_batch_prompts(prompts: list[str], n_outputs: int = 1, model_name: str = 'mistral-small-latest') -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Sends a batch of text prompts to the Gemini API asynchronously and returns \n",
    "    the results as a list of lists.\n",
    "\n",
    "    Args:\n",
    "        prompts: A list of string prompts to send to the model.\n",
    "        n_outputs: The number of distinct outputs (candidates) to generate for each prompt.\n",
    "        model_name: The model to use for the batch job.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the n_outputs \n",
    "        generated texts for the corresponding input prompt.\n",
    "    \"\"\"\n",
    "    if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "        raise RuntimeError(\"Error: MISTRAL_API_KEY environment variable is not set.\")\n",
    "    api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    # print(\n",
    "    #     f\"Starting {len(prompts)} parallel requests, generating {n_outputs} candidates each...\")\n",
    "\n",
    "    # Create a list of all asynchronous tasks\n",
    "    tasks = [\n",
    "        generate_content_for_prompt(client, prompt, n_outputs, model_name)\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    # Run all tasks concurrently and wait for them to complete\n",
    "    results_list_of_lists = await asyncio.gather(*tasks)\n",
    "\n",
    "    return results_list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4c18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "async def process_batch_chunks(sample_df: pd.DataFrame, nikud_prompt: str, n_outputs: int = 5):\n",
    "    \"\"\"\n",
    "    Asynchronous function to process prompts in chunks.\n",
    "\n",
    "    Args:\n",
    "        sample_df: The DataFrame containing the 'text' column to process.\n",
    "        nikud_prompt: The prefix to add to each prompt.\n",
    "        chunk_size: The number of prompts to send in one batch.\n",
    "        n_outputs: The number of candidate responses to request per prompt.\n",
    "    \"\"\"\n",
    "    # Prepare all prompts\n",
    "    prompt_data = list(zip(sample_df['text'].index, [\n",
    "                       nikud_prompt + strip_nikud(txt) for txt in sample_df['text'].tolist()]))\n",
    "    sample_df['uncertainty_word_mask'] = pd.Series(\n",
    "        [None] * len(sample_df), index=sample_df.index, dtype='object')\n",
    "\n",
    "    for i in range(0, len(prompt_data)):\n",
    "        start = time.time()\n",
    "        index, prompt = prompt_data[i]\n",
    "\n",
    "        res = await run_async_batch_prompts([prompt], n_outputs=n_outputs)\n",
    "\n",
    "        uncertainty_word_mask = compute_word_entropy_mask(\n",
    "            # Get the original text using the actual index\n",
    "            sample_df.loc[index, 'text'],\n",
    "            # Get the corresponding result from the batch response\n",
    "            res[0],\n",
    "            threshold=0.0\n",
    "        )[0]\n",
    "\n",
    "        # if 1 not in uncertainty_word_mask:\n",
    "        #     print(\n",
    "        #         f\"Warning: No uncertain words found for index {index}. Mask: {uncertainty_word_mask}, Results: {res[0]}\")\n",
    "        sample_df.loc[index, 'uncertainty_word_mask'] = str(uncertainty_word_mask)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        # If the request took less than 1s, wait the remainder\n",
    "        if elapsed < 1:\n",
    "            time.sleep(1 - elapsed)\n",
    "\n",
    "    # return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec5995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n",
      "Starting 1 parallel requests, generating 5 candidates each...\n"
     ]
    }
   ],
   "source": [
    "await process_batch_chunks(hewiki_sample, nikud_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikud-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
