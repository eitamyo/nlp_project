{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c72b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (1.9.10)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (2.11.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (6.0.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from mistralai) (0.4.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pydantic>=2.10.3->mistralai) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f387c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8e2bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f09edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BaseModelCard(id='mistral-medium-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-medium-2505', description='Our frontier-class multimodal model released May 2025.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-large-latest', description='Official mistral-large-latest Mistral AI model', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-latest', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-medium', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='ministral-3b-2410', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758799916, owned_by='mistralai', name='ministral-3b-2410', description='Official ministral-3b-2410 Mistral AI model', max_context_length=32768, aliases=['ministral-3b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='ministral-3b-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758799916, owned_by='mistralai', name='ministral-3b-2410', description='Official ministral-3b-2410 Mistral AI model', max_context_length=32768, aliases=['ministral-3b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='ministral-8b-2410', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='ministral-8b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-7b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['mistral-tiny', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-2312', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-nemo', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mistral-nemo-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-tiny-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x7b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['mistral-small', 'mistral-small-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['open-mixtral-8x7b', 'mistral-small-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2312', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mixtral-8x7b', description='Our first sparse mixture-of-experts released December 2023.', max_context_length=32768, aliases=['open-mixtral-8x7b', 'mistral-small'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x22b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mixtral-8x22b', description='Our best open source model to date released April 2024. ', max_context_length=65536, aliases=['open-mixtral-8x22b-2404'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='open-mixtral-8x22b-2404', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='open-mixtral-8x22b', description='Our best open source model to date released April 2024. ', max_context_length=65536, aliases=['open-mixtral-8x22b'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-small-2409', description='Our latest enterprise-grade small model with the latest version v2 released September 2024. ', max_context_length=32768, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-large-2407', description='Our top-tier reasoning model for high-complexity tasks with the latest version v2 released July 2024.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-large-2411', description='Our top-tier reasoning model for high-complexity tasks with the lastest version released November 2024.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-latest', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='mistral-large-pixtral-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'pixtral-large-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2412', 'codestral-2411-rc5'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2412', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2501', 'codestral-2411-rc5'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2411-rc5', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=262144, aliases=['codestral-2501', 'codestral-2412'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='codestral-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-2508'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='devstral-small-2505', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='devstral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='pixtral-12b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-small-2501', description='Our latest enterprise-grade small model with the latest version released January 2025. ', max_context_length=32768, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2503', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-small-2503', description='Our latest enterprise-grade small model with the latest version released March 2025.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-2506'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-saba-2502', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-saba-2502', description='Official mistral-saba-2502 Mistral AI model', max_context_length=32768, aliases=['mistral-saba-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='mistral-saba-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-saba-2502', description='Official mistral-saba-2502 Mistral AI model', max_context_length=32768, aliases=['mistral-saba-2502'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-medium-2506', description='Our frontier-class reasoning model released June 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-medium-2507', description='Our frontier-class reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-small-2506', description='Our efficient reasoning model released June 2025.', max_context_length=40000, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-small-2507', description='Our efficient reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='magistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'),\n",
       " BaseModelCard(id='mistral-embed-2312', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='codestral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='codestral-embed-2505', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-moderation-2411', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758799916, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-moderation-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1758799916, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-2503', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-ocr-2503', description='Official mistral-ocr-2503 Mistral AI model', max_context_length=16384, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='mistral-ocr-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1758799916, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-transcribe-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'),\n",
       " BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1758799916, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "client.models.list().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b75c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "nikud_prompt = \"\"\"\n",
    "Add full Hebrew nikud (vowel marks) to the following text. \n",
    "Return only the hebrew text with nikud. Do not provide explanations or notes, or any other text other than the hebrew one. \n",
    "Text: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc0c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['מְמַלְכַּת רוֹהַן הָיְתָה מַמְלָכָה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל־יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעוֹת\".', 'מְמַלְכַּת רוֹהַן הָיְתָה מַמְלָכָה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל-יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטֶּבַע\".', 'מֶמְלֶכֶת רוֹהָן הָיְתָה מַמְלָכָה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעֹת\".', 'מְמַלְכַּת רוֹהָן הָיְתָה מַמְלָכָה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטַּבְעוּת\".', 'מְמַלְכֶת רוֹהַן הָיְתָה מְמַלְכָה בְּדִיּוֹנִית שֶׁנִּצְרְעָה עַל יְדֵי ג\\'.ר.ר. טוֹלְקִין בְּסִפְרוֹ \"שַׂר הַטֶּבַע\".']\n"
     ]
    }
   ],
   "source": [
    "# model = \"codestral-2501\"\n",
    "model = \"mistral-small-latest\"\n",
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    n=5,\n",
    "    temperature=0.6,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": nikud_prompt + \"ממלכת רוהן הייתה ממלכה בדיונית שנוצרה על ידי ג'.ר.ר. טולקין בספרו \\\"שר הטבעות\\\".\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print([choice.message.content for choice in chat_response.choices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfda9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv('./datasets/hewiki/hebrew_nikud_dataset_filtered_word_mask.csv')\n",
    "hewiki_sample = filtered_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d48ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>nikud_mask</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>nikud_mask_length</th>\n",
       "      <th>nikud_word_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61034</th>\n",
       "      <td>ב-20 בספטמבר, אחרי שהעמק נוקה ממטעני חבלה, כוח...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>התנועה הדרומית</td>\n",
       "      <td>1526</td>\n",
       "      <td>124</td>\n",
       "      <td>119</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>חוקר הקבלה והשבתאות גרשום שלום טוען שאף על פי ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>היסטוריה של עם ישראל</td>\n",
       "      <td>7494</td>\n",
       "      <td>256</td>\n",
       "      <td>257</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16513</th>\n",
       "      <td>הם הקימו מגדל בשם \"שאכּתאכּי\", ששימש כמצפה כוכ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>נריקלה</td>\n",
       "      <td>239</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>מערכת גִּ'ין-צְ'װַאן (השנייה)</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>צ'יינלונג, קיסר סין</td>\n",
       "      <td>6765</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29165</th>\n",
       "      <td>בשנת 1907 גילה עובד מחצבה במאוּאֶר, כפר ליד הי...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>הומו היידלברגנסיס</td>\n",
       "      <td>4382</td>\n",
       "      <td>166</td>\n",
       "      <td>162</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "61034  ב-20 בספטמבר, אחרי שהעמק נוקה ממטעני חבלה, כוח...   \n",
       "827    חוקר הקבלה והשבתאות גרשום שלום טוען שאף על פי ...   \n",
       "16513  הם הקימו מגדל בשם \"שאכּתאכּי\", ששימש כמצפה כוכ...   \n",
       "4276                       מערכת גִּ'ין-צְ'װַאן (השנייה)   \n",
       "29165  בשנת 1907 גילה עובד מחצבה במאוּאֶר, כפר ליד הי...   \n",
       "\n",
       "                                              nikud_mask  \\\n",
       "61034  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "827    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16513  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4276   [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "29165  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "              article_title  article_length  text_length  nikud_mask_length  \\\n",
       "61034        התנועה הדרומית            1526          124                119   \n",
       "827    היסטוריה של עם ישראל            7494          256                257   \n",
       "16513                נריקלה             239           78                 78   \n",
       "4276    צ'יינלונג, קיסר סין            6765           29                 27   \n",
       "29165     הומו היידלברגנסיס            4382          166                162   \n",
       "\n",
       "                                         nikud_word_mask  \n",
       "61034  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "827    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "16513            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4276                                           [0, 1, 0]  \n",
       "29165  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hewiki_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_word_entropy_mask(original_sentence: str, outputs: list[str], threshold: float = 0.0):\n",
    "    \"\"\"\n",
    "    Compute entropy-based ambiguity mask for words in a sentence.\n",
    "    \n",
    "    Args:\n",
    "        original_sentence (str): input sentence without nikud (used for tokenization).\n",
    "        outputs (list[str]): list of generated outputs with nikud (multiple generations).\n",
    "        threshold (float): entropy threshold above which a word is marked ambiguous.\n",
    "                           Default = 0.0 (any disagreement → ambiguous).\n",
    "    \n",
    "    Returns:\n",
    "        mask (list[int]): 0/1 mask, length = number of words in original_sentence.\n",
    "                          1 = ambiguous (uncertain), 0 = consistent (confident).\n",
    "        entropies (list[float]): entropy per word (in bits).\n",
    "    \"\"\"\n",
    "    # Split original text into words (no nikud, for alignment)\n",
    "    words = original_sentence.split()\n",
    "    n_words = len(words)\n",
    "\n",
    "    # Collect nikud-versions per word across outputs\n",
    "    word_variants = [ [] for _ in range(n_words) ]\n",
    "    for out in outputs:\n",
    "        out_words = out.split()\n",
    "        if len(out_words) != n_words:\n",
    "            # simple alignment fallback: skip misaligned outputs\n",
    "            continue\n",
    "        for i, w in enumerate(out_words):\n",
    "            word_variants[i].append(w)\n",
    "\n",
    "    entropies = []\n",
    "    mask = []\n",
    "    for variants in word_variants:\n",
    "        if not variants:\n",
    "            entropies.append(0.0)\n",
    "            mask.append(0)\n",
    "            continue\n",
    "        counts = Counter(variants)\n",
    "        total = sum(counts.values())\n",
    "        probs = [c/total for c in counts.values()]\n",
    "        entropy = -sum(p * math.log2(p) for p in probs)\n",
    "        entropies.append(entropy)\n",
    "        mask.append(1 if entropy > threshold else 0)\n",
    "\n",
    "    return mask, entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63c7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hebrew_tokenizer as ht\n",
    "\n",
    "def strip_nikud(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove Hebrew nikud (vowel marks) from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input Hebrew text with nikud.\n",
    "    \"\"\"\n",
    "    return ht.NIKUD_PATTERN.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfadbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed32277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def generate_content_for_prompt(client: Mistral, prompt: str, n_outputs: int, model_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    An asynchronous helper function to call the API for a single prompt \n",
    "    and extract the generated text.\n",
    "    \"\"\"\n",
    "    if prompt in completion_cache:\n",
    "        return completion_cache[prompt]\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # Use the asynchronous client module (client.aio)\n",
    "            chat_response = await client.chat.complete_async(\n",
    "                model=model_name,\n",
    "                n=n_outputs,\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                timeout_ms=60000\n",
    "            )\n",
    "\n",
    "            # print(f\"Completed request\")\n",
    "\n",
    "            # Extract the text from each candidate\n",
    "            results = [\n",
    "                choice.message.content for choice in chat_response.choices]\n",
    "            completion_cache[prompt] = results\n",
    "            elapsed = time.time() - start\n",
    "            if elapsed < 1:\n",
    "                time.sleep(1 - elapsed)\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                print(\n",
    "                    f\"Quota exceeded {e}. Waiting for 30 seconds before retrying...\")\n",
    "                await asyncio.sleep(30)  # Wait before retrying\n",
    "                continue  # Retry the request\n",
    "            # print(f\"Request failed with error: {e}\")\n",
    "            # Return error message for a failed request\n",
    "            retries += 1\n",
    "            if retries < 3:\n",
    "                continue\n",
    "            raise RuntimeError(f\"Request failed with error after 3 retries: {e}\")\n",
    "\n",
    "\n",
    "async def run_async_batch_prompts(prompts: list[str], n_outputs: int = 1, model_name: str = 'mistral-small-latest') -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Sends a batch of text prompts to the Gemini API asynchronously and returns \n",
    "    the results as a list of lists.\n",
    "\n",
    "    Args:\n",
    "        prompts: A list of string prompts to send to the model.\n",
    "        n_outputs: The number of distinct outputs (candidates) to generate for each prompt.\n",
    "        model_name: The model to use for the batch job.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the n_outputs \n",
    "        generated texts for the corresponding input prompt.\n",
    "    \"\"\"\n",
    "    if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "        raise RuntimeError(\"Error: MISTRAL_API_KEY environment variable is not set.\")\n",
    "    api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    # print(\n",
    "    #     f\"Starting {len(prompts)} parallel requests, generating {n_outputs} candidates each...\")\n",
    "\n",
    "    # Create a list of all asynchronous tasks\n",
    "    tasks = [\n",
    "        generate_content_for_prompt(client, prompt, n_outputs, model_name)\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    # Run all tasks concurrently and wait for them to complete\n",
    "    results_list_of_lists = await asyncio.gather(*tasks)\n",
    "\n",
    "    return results_list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def process_batch_chunks(sample_df: pd.DataFrame, nikud_prompt: str, n_outputs: int = 5):\n",
    "    \"\"\"\n",
    "    Asynchronous function to process prompts in chunks.\n",
    "\n",
    "    Args:\n",
    "        sample_df: The DataFrame containing the 'text' column to process.\n",
    "        nikud_prompt: The prefix to add to each prompt.\n",
    "        chunk_size: The number of prompts to send in one batch.\n",
    "        n_outputs: The number of candidate responses to request per prompt.\n",
    "    \"\"\"\n",
    "    # Prepare all prompts\n",
    "    prompt_data = list(zip(sample_df['text'].index, [\n",
    "                       nikud_prompt + strip_nikud(txt) for txt in sample_df['text'].tolist()]))\n",
    "    sample_df['uncertainty_word_mask'] = pd.Series(\n",
    "        [None] * len(sample_df), index=sample_df.index, dtype='object')\n",
    "\n",
    "    for i in tqdm(range(0, len(prompt_data))):\n",
    "        index, prompt = prompt_data[i]\n",
    "\n",
    "        res = await run_async_batch_prompts([prompt], n_outputs=n_outputs)\n",
    "\n",
    "        uncertainty_word_mask = compute_word_entropy_mask(\n",
    "            # Get the original text using the actual index\n",
    "            sample_df.loc[index, 'text'],\n",
    "            # Get the corresponding result from the batch response\n",
    "            res[0],\n",
    "            threshold=0.0\n",
    "        )[0]\n",
    "\n",
    "        # if 1 not in uncertainty_word_mask:\n",
    "        #     print(\n",
    "        #         f\"Warning: No uncertain words found for index {index}. Mask: {uncertainty_word_mask}, Results: {res[0]}\")\n",
    "        sample_df.loc[index, 'uncertainty_word_mask'] = str(uncertainty_word_mask)\n",
    "        \n",
    "        # elapsed = time.time() - start\n",
    "        # # If the request took less than 1s, wait the remainder\n",
    "        # if elapsed < 1:\n",
    "        #     time.sleep(1 - elapsed)\n",
    "\n",
    "    # return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac45427",
   "metadata": {},
   "outputs": [],
   "source": [
    "hewiki_sample.iloc[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdec5995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 504/1000 [01:22<25:02,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 545/1000 [03:36<19:48,  2.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [24:25<00:00,  1.47s/it] \n"
     ]
    }
   ],
   "source": [
    "await process_batch_chunks(hewiki_sample, nikud_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac2e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Compute the overlap ratio in a separate step\n",
    "def overlap_ratio(row):\n",
    "    uncertainty_mask = row['uncertainty_word_mask']\n",
    "    if isinstance(uncertainty_mask, str):\n",
    "        uncertainty_mask = ast.literal_eval(uncertainty_mask)\n",
    "    nikud_mask = row['nikud_word_mask']\n",
    "    if isinstance(nikud_mask, str):\n",
    "        nikud_mask = ast.literal_eval(nikud_mask)\n",
    "    overlap = sum(1 for u, n in zip(uncertainty_mask, nikud_mask) if u == 1 and n == 1)\n",
    "    total_nikud = sum(nikud_mask)\n",
    "    return overlap / total_nikud if total_nikud > 0 else None\n",
    "\n",
    "def opposite_overlap_ratio(row):\n",
    "    uncertainty_mask = row['uncertainty_word_mask']\n",
    "    if isinstance(uncertainty_mask, str):\n",
    "        uncertainty_mask = ast.literal_eval(uncertainty_mask)\n",
    "    nikud_mask = row['nikud_word_mask']\n",
    "    if isinstance(nikud_mask, str):\n",
    "        nikud_mask = ast.literal_eval(nikud_mask)\n",
    "    overlap = sum(1 for u, n in zip(uncertainty_mask, nikud_mask) if u == 1 and n == 1)\n",
    "    total_uncertainty = sum(uncertainty_mask)\n",
    "    return overlap / total_uncertainty if total_uncertainty > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2795809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    951.000000\n",
       "mean       0.490411\n",
       "std        0.486863\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: uncertainty_overlap_ratio, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hewiki_sample['uncertainty_overlap_ratio'] = hewiki_sample.apply(overlap_ratio, axis=1)\n",
    "hewiki_sample['uncertainty_overlap_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b5dc029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    765.000000\n",
       "mean       0.118056\n",
       "std        0.177875\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.076923\n",
       "75%        0.142857\n",
       "max        1.000000\n",
       "Name: uncertainty_overlap_ratio_2, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hewiki_sample['uncertainty_overlap_ratio_2'] = hewiki_sample.apply(opposite_overlap_ratio, axis=1)\n",
    "hewiki_sample['uncertainty_overlap_ratio_2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7efa1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "benyehuda_df = pd.read_csv('./datasets/projectbenyehuda/benyehuda_nikud_dataset_with_uncertainty_word_mask.csv')\n",
    "by_sample = benyehuda_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9bc6da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                   45925\n",
       "title                                                  הספרות ורוח המוזיקה\n",
       "text                     כשהוא מזהה כשְׁרון מוזיקלי אצל הנערה עם הסוודר...\n",
       "nikud_mask               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "text_length                                                            302\n",
       "nikud_mask_length                                                      301\n",
       "uncertainty_mask         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "nikud_word_mask          [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "uncertainty_word_mask    [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, ...\n",
       "Name: 4794, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_sample.iloc[503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ceb767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 582/1000 [03:37<17:18,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 675/1000 [07:58<10:51,  2.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 791/1000 [13:51<15:18,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 815/1000 [15:24<09:23,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 886/1000 [19:03<07:57,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 932/1000 [21:32<02:41,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 971/1000 [23:35<01:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n",
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 972/1000 [24:38<09:52, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 993/1000 [26:04<00:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}. Waiting for 30 seconds before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [26:44<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "await process_batch_chunks(by_sample, nikud_prompt, n_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5549c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hewiki_sample.to_csv('./datasets/hewiki/hebrew_nikud_dataset_sample_mistral.csv', index=False)\n",
    "by_sample.to_csv('./datasets/projectbenyehuda/benyehuda_nikud_dataset_sample_mistral.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1021195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    970.000000\n",
       "mean       0.503848\n",
       "std        0.467204\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: uncertainty_overlap_ratio, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_sample['uncertainty_overlap_ratio'] = by_sample.apply(overlap_ratio, axis=1)\n",
    "by_sample['uncertainty_overlap_ratio'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikud-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
