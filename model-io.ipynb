{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18db031b",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import torch and the necessary classes from the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08362824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e297a",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model\n",
    "Load the AutoTokenizer and AutoModel for 'dicta-il/dictabert-large-char-menaked'. Set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58436999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForDiacritization(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(1024, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (menaked): BertMenakedHead(\n",
       "    (nikud_cls): Linear(in_features=1024, out_features=29, bias=True)\n",
       "    (shin_cls): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"dicta-il/dictabert-large-char-menaked\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a97c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_probs(sentences, tokenizer, model, mark_matres_lectionis=None, top_k=5):\n",
    "    # Use model.predict for decoded output\n",
    "    decoded = model.predict(sentences, tokenizer, mark_matres_lectionis=mark_matres_lectionis)\n",
    "    \n",
    "    # Manual forward pass\n",
    "    inputs = tokenizer(sentences, padding='longest', truncation=True,\n",
    "                       return_tensors='pt', return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(**inputs, return_dict=True)\n",
    "    logits = outputs.logits  # MenakedLogitsOutput\n",
    "    nikud_logits = logits.nikud_logits  # [batch, seq_len, num_nikud]\n",
    "    shin_logits = logits.shin_logits    # [batch, seq_len, num_shin]\n",
    "    \n",
    "    results = []\n",
    "    for sent_idx, (sentence, offsets) in enumerate(zip(sentences, offset_mapping)):\n",
    "        # For each character token, collect top-k probabilities\n",
    "        sent_data = {'decoded': decoded[sent_idx], 'chars': []}\n",
    "        probs = torch.softmax(nikud_logits[sent_idx], dim=-1)  # probabilities\n",
    "        \n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if end - start != 1:\n",
    "                continue\n",
    "            char = sentence[start:end]\n",
    "            dist = probs[i]\n",
    "            top_p, top_ids = torch.topk(dist, top_k)\n",
    "            sent_data['chars'].append({\n",
    "                'char': char,\n",
    "                'predictions': {\n",
    "                    model.config.nikud_classes[label_id.item()]: float(p.item())\n",
    "                    for p, label_id in zip(top_p, top_ids)\n",
    "                }\n",
    "            })\n",
    "        results.append(sent_data)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc071d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chars': [{'char': 'מ',\n",
      "            'predictions': {'ְ': 0.0015557006699964404,\n",
      "                            'ַ': 0.9960278272628784,\n",
      "                            'ָ': 0.0013132020831108093}},\n",
      "           {'char': 'ס',\n",
      "            'predictions': {'ֶ': 0.000505781383253634,\n",
      "                            'ֵּ': 0.001638155896216631,\n",
      "                            'ֶּ': 0.9971392154693604}},\n",
      "           {'char': 'כ',\n",
      "            'predictions': {'ֶ': 0.999430239200592,\n",
      "                            'ֶּ': 0.00017696806753519922,\n",
      "                            'ַּ': 0.0001835094444686547}},\n",
      "           {'char': 'ת',\n",
      "            'predictions': {'': 0.9999827146530151,\n",
      "                            'ּ': 2.610164528960013e-06,\n",
      "                            'ְּ': 4.128542968828697e-06}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4220789074897766,\n",
      "                            'ֵ': 0.13712063431739807,\n",
      "                            'ָ': 0.24417512118816376}},\n",
      "           {'char': 'ע',\n",
      "            'predictions': {'ֲ': 0.9481474161148071,\n",
      "                            'ֶ': 0.03058251366019249,\n",
      "                            'ַ': 0.006011860445141792}},\n",
      "           {'char': 'ר',\n",
      "            'predictions': {'ְ': 0.03296397626399994,\n",
      "                            'ֵ': 0.17446018755435944,\n",
      "                            'ָ': 0.7811325192451477}},\n",
      "           {'char': 'ל',\n",
      "            'predictions': {'': 0.0017933935159817338,\n",
      "                            'ָ': 0.9795772433280945,\n",
      "                            'ָּ': 0.01438802108168602}},\n",
      "           {'char': 'ה',\n",
      "            'predictions': {'': 0.9997889399528503,\n",
      "                            'ּ': 0.00017809603014029562,\n",
      "                            'ַּ': 1.0864166142710019e-05}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4355073869228363,\n",
      "                            'ֵ': 0.13366077840328217,\n",
      "                            'ָ': 0.22296445071697235}},\n",
      "           {'char': 'ה',\n",
      "            'predictions': {'': 2.4647548343637027e-05,\n",
      "                            'ִ': 0.9999408721923828,\n",
      "                            'ֵ': 1.4262688637245446e-05}},\n",
      "           {'char': 'י',\n",
      "            'predictions': {'': 0.9999815225601196,\n",
      "                            'ּ': 3.2005964385461994e-06,\n",
      "                            'ְּ': 3.6326562167232623e-06}},\n",
      "           {'char': 'א',\n",
      "            'predictions': {'': 0.9999847412109375,\n",
      "                            '<MAT_LECT>': 3.2585701319476357e-06,\n",
      "                            'ָ': 1.1659327583402046e-06}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4296160936355591,\n",
      "                            'ֵ': 0.13330817222595215,\n",
      "                            'ָ': 0.22697101533412933}},\n",
      "           {'char': 'ה',\n",
      "            'predictions': {'': 0.00015306386922020465,\n",
      "                            'ִ': 3.1923420465318486e-05,\n",
      "                            'ַ': 0.9997132420539856}},\n",
      "           {'char': 'מ',\n",
      "            'predictions': {'ַ': 0.0016620312817394733,\n",
      "                            'ַּ': 0.9925801157951355,\n",
      "                            'ָּ': 0.002950961235910654}},\n",
      "           {'char': 'ס',\n",
      "            'predictions': {'ֶ': 0.0015248394338414073,\n",
      "                            'ֵּ': 5.1344646635698155e-05,\n",
      "                            'ֶּ': 0.9981814622879028}},\n",
      "           {'char': 'כ',\n",
      "            'predictions': {'': 5.635829074890353e-05,\n",
      "                            'ֶ': 0.9995822310447693,\n",
      "                            'ֶּ': 0.00016196959768421948}},\n",
      "           {'char': 'ת',\n",
      "            'predictions': {'': 0.9999600648880005,\n",
      "                            'ּ': 4.9753321036405396e-06,\n",
      "                            'ְּ': 1.984272967092693e-05}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4330575466156006,\n",
      "                            'ֵ': 0.13127385079860687,\n",
      "                            'ָ': 0.22469976544380188}},\n",
      "           {'char': 'ה',\n",
      "            'predictions': {'ֶ': 0.0009354205103591084,\n",
      "                            'ַ': 0.0004261312715243548,\n",
      "                            'ָ': 0.9983539581298828}},\n",
      "           {'char': 'ע',\n",
      "            'predictions': {'ְ': 0.00014725745131727308,\n",
      "                            'ֲ': 0.999506950378418,\n",
      "                            'ֳ': 0.00014955743972677737}},\n",
      "           {'char': 'ש',\n",
      "            'predictions': {'ִ': 0.9440317153930664,\n",
      "                            'ֵ': 0.020574379712343216,\n",
      "                            'ִּ': 0.008630547672510147}},\n",
      "           {'char': 'י',\n",
      "            'predictions': {'': 0.9997636675834656,\n",
      "                            '<MAT_LECT>': 0.00018665719835553318,\n",
      "                            'ְּ': 1.0349141120968852e-05}},\n",
      "           {'char': 'ר',\n",
      "            'predictions': {'': 2.3156586394179612e-05,\n",
      "                            'ִ': 0.9999445676803589,\n",
      "                            'ִּ': 1.3527743249142077e-05}},\n",
      "           {'char': 'י',\n",
      "            'predictions': {'': 0.9998511075973511,\n",
      "                            '<MAT_LECT>': 2.5653967895777896e-05,\n",
      "                            'ּ': 3.807883331319317e-05}},\n",
      "           {'char': 'ת',\n",
      "            'predictions': {'': 0.9999874830245972,\n",
      "                            'ָ': 1.367811478303338e-06,\n",
      "                            'ְּ': 1.8693253878154792e-06}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4381777346134186,\n",
      "                            'ֵ': 0.13982246816158295,\n",
      "                            'ָ': 0.24279843270778656}},\n",
      "           {'char': 'ב',\n",
      "            'predictions': {'ְּ': 0.9957018494606018,\n",
      "                            'ִּ': 0.0006775945075787604,\n",
      "                            'ַּ': 0.0030813091434538364}},\n",
      "           {'char': 'ס',\n",
      "            'predictions': {'ֵ': 0.9951044321060181,\n",
      "                            'ֶ': 0.00028403737815096974,\n",
      "                            'ֵּ': 0.004274449311196804}},\n",
      "           {'char': 'ד',\n",
      "            'predictions': {'ֶ': 0.9987719655036926,\n",
      "                            'ַ': 0.00030012475326657295,\n",
      "                            'ֶּ': 0.00032927977736108005}},\n",
      "           {'char': 'ר',\n",
      "            'predictions': {'': 0.9999688863754272,\n",
      "                            'ְ': 3.466185034994851e-06,\n",
      "                            'ּ': 4.75501337859896e-06}},\n",
      "           {'char': ' ',\n",
      "            'predictions': {'': 0.4306861162185669,\n",
      "                            'ֵ': 0.13675707578659058,\n",
      "                            'ָ': 0.22632752358913422}},\n",
      "           {'char': 'ז',\n",
      "            'predictions': {'ְ': 0.9972242116928101,\n",
      "                            'ָ': 0.0002814083709381521,\n",
      "                            'ְּ': 0.0014658503932878375}},\n",
      "           {'char': 'ר',\n",
      "            'predictions': {'ְ': 0.00012318075459916145,\n",
      "                            'ֵ': 0.00012907730706501752,\n",
      "                            'ָ': 0.9995880722999573}},\n",
      "           {'char': 'ע',\n",
      "            'predictions': {'': 3.768541500903666e-05,\n",
      "                            'ִ': 0.9998968839645386,\n",
      "                            'ַ': 2.310593845322728e-05}},\n",
      "           {'char': 'י',\n",
      "            'predictions': {'': 0.9998866319656372,\n",
      "                            '<MAT_LECT>': 7.0831029006512836e-06,\n",
      "                            'ִ': 7.020958582870662e-05}},\n",
      "           {'char': 'ם',\n",
      "            'predictions': {'': 0.9989811778068542,\n",
      "                            'ְ': 9.955267887562513e-05,\n",
      "                            'ָ': 0.0006605623057112098}}],\n",
      " 'decoded': 'מַסֶּכֶת עֲרָלָה הִיא הַמַּסֶּכֶת הָעֲשִׂירִית בְּסֵדֶר זְרָעִים'}\n"
     ]
    }
   ],
   "source": [
    "res = predict_with_probs(['מסכת עָרְלָה היא המסכת העשירית בסדר זרעים'], tokenizer, model, top_k=3)\n",
    "import pprint; pprint.pprint(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def nikud_uncertainty(text, model, tokenizer, \n",
    "                      top_k=5, \n",
    "                      entropy_threshold=1.0, \n",
    "                      margin_threshold=0.2, \n",
    "                      maxprob_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Identify ambiguous characters in the text according to nikud predictions.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    offsets = inputs.pop(\"offset_mapping\")[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)   # MenakedOutput\n",
    "        probs = torch.softmax(outputs.logits.nikud_logits[0], dim=-1)  # shape [seq_len, num_nikud_classes]\n",
    "\n",
    "    id2label = model.config.nikud_classes  # list of all nikud symbols\n",
    "\n",
    "    ambiguous = []\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if end - start != 1:  # skip special tokens / padding\n",
    "            continue\n",
    "        char = text[start:end]\n",
    "        dist = probs[i]\n",
    "\n",
    "        # Sort probabilities\n",
    "        sorted_probs, sorted_ids = torch.sort(dist, descending=True)\n",
    "        p1, p2 = sorted_probs[0].item(), sorted_probs[1].item()\n",
    "\n",
    "        # --- criteria ---\n",
    "        entropy = -sum(p.item() * log2(p.item()) for p in dist if p.item() > 0)\n",
    "        margin = p1 - p2\n",
    "        max_prob = p1\n",
    "\n",
    "        # decide ambiguity\n",
    "        is_ambig = (entropy > entropy_threshold) or (margin < margin_threshold) or (max_prob < maxprob_threshold)\n",
    "\n",
    "        if is_ambig:\n",
    "            ambiguous.append({\n",
    "                \"char\": char,\n",
    "                \"position\": (start, end),\n",
    "                \"entropy\": entropy,\n",
    "                \"margin\": margin,\n",
    "                \"max_prob\": max_prob,\n",
    "                \"top_candidates\": [\n",
    "                    (id2label[sorted_ids[j].item()], sorted_probs[j].item())\n",
    "                    for j in range(min(top_k, len(sorted_ids)))\n",
    "                ]\n",
    "            })\n",
    "    return ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df30e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char': ' ', 'position': (tensor(4), tensor(5)), 'entropy': 2.520866388614735, 'margin': 0.10133625566959381, 'max_prob': 0.3462553322315216, 'top_candidates': [('ָ', 0.3462553322315216), ('ֵ', 0.2449190765619278), ('', 0.2309339940547943), ('ִ', 0.0470949187874794), ('<MAT_LECT>', 0.0413050502538681)]}\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "ambig = nikud_uncertainty(\"שלום עולם\", model, tokenizer)\n",
    "for a in ambig:\n",
    "    print(a)\n",
    "    print(a[\"position\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041612dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a948ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eitam\\miniconda3\\envs\\nikud-bert\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192c7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitam\\AppData\\Local\\Temp\\ipykernel_28396\\2670551155.py:75: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = matplotlib.cm.get_cmap(\"YlOrRd\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color:#fff0a7'>ד</span><span style='color:#ffffcc'>ה</span><span style='color:#800026'> </span><span style='color:#800026'>ה</span><span style='color:#ffe691'>ו</span><span style='color:#c00225'>ו</span><span style='color:#fffdc8'>ש</span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>,</span><span style='color:#800026'> </span><span style='color:#ffffcc'>ש</span><span style='color:#ffffcc'>ה</span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>ה</span><span style='color:#800026'> </span><span style='color:#800026'>כ</span><span style='color:#ffffcc'>י</span><span style='color:#fec662'>מ</span><span style='color:#ffffcc'>א</span><span style='color:#ffffcc'>י</span><span style='color:#800026'> </span><span style='color:#ffffcc'>פ</span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>ז</span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>ק</span><span style='color:#ffffcc'>ל</span><span style='color:#ffffcc'>י</span><span style='color:#800026'> </span><span style='color:#ffffcc'>ה</span><span style='color:#ffffcc'>ו</span><span style='color:#ffffcc'>נ</span><span style='color:#ffffcc'>ג</span><span style='color:#ffffcc'>ר</span><span style='color:#ffffcc'>י</span><span style='color:#800026'> </span><span style='color:#ffffcc'>מ</span><span style='color:#ffffcc'>מ</span><span style='color:#ffffcc'>ו</span><span style='color:#ffffcc'>צ</span><span style='color:#ffffcc'>א</span><span style='color:#800026'> </span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>ה</span><span style='color:#ffffcc'>ו</span><span style='color:#ffffcc'>ד</span><span style='color:#ffffcc'>י</span><span style='color:#ffffcc'>,</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color:#800026'>ש</span><span style='color:#fea546'>מ</span><span style='color:#ffffcc'>ח</span><span style='color:#ffffcc'>ה</span><span style='color:#ffeea3'> </span><span style='color:#ffffcc'>ה</span><span style='color:#ffffcc'>א</span><span style='color:#ffffcc'>ח</span><span style='color:#ffeda0'> </span><span style='color:#fff7b7'>ח</span><span style='color:#ffffcc'>ב</span><span style='color:#ffffcc'>ר</span><span style='color:#ffea9b'> </span><span style='color:#ffffcc'>ט</span><span style='color:#ffffcc'>ו</span><span style='color:#ffffcc'>ב</span><span style='color:#ffefa4'> </span><span style='color:#ffffcc'>ש</span><span style='color:#ffffcc'>ל</span><span style='color:#ffeda1'> </span><span style='color:#800026'>כ</span><span style='color:#c30424'>ד</span><span style='color:#ffe794'>ר</span><span style='color:#fec662'>ל</span><span style='color:#fec561'>ע</span><span style='color:#800026'>ו</span><span style='color:#f33b25'>מ</span><span style='color:#ffffcc'>ר</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import matplotlib\n",
    "import math\n",
    "\n",
    "def colorize_text_by_certainty(text, model, tokenizer,\n",
    "                               certainty_metric=\"max_prob\",\n",
    "                               combine=\"max\",   # \"max\" or \"avg\"\n",
    "                               scale=\"linear\",\n",
    "                               low_conf=0.6, high_conf=0.95):\n",
    "    \"\"\"\n",
    "    Display text with characters color-coded by combined certainty\n",
    "    from both nikud_logits and shin_logits.\n",
    "    \n",
    "    - certainty_metric: \"max_prob\" or \"entropy\"\n",
    "    - combine: how to combine nikud & shin uncertainty (\"max\" or \"avg\")\n",
    "    - scale: \"linear\", \"sqrt\", or \"log\"\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    offsets = inputs.pop(\"offset_mapping\")[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        nikud_probs = torch.softmax(outputs.logits.nikud_logits[0], dim=-1)\n",
    "        shin_probs  = torch.softmax(outputs.logits.shin_logits[0], dim=-1)\n",
    "\n",
    "    html_chars = []\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if end - start != 1:\n",
    "            continue\n",
    "        char = text[start:end]\n",
    "        dist = nikud_probs[i]\n",
    "\n",
    "        # ---- nikud certainty ----\n",
    "        if certainty_metric == \"max_prob\":\n",
    "            nikud_conf = dist.max().item()\n",
    "        elif certainty_metric == \"entropy\":\n",
    "            entropy = -sum(p.item() * math.log2(p.item()) for p in dist if p.item() > 0)\n",
    "            nikud_conf = 1 - entropy / math.log2(len(dist))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown certainty metric\")\n",
    "\n",
    "        # ---- shin certainty (only for ש) ----\n",
    "        if char == \"ש\":\n",
    "            s_probs = shin_probs[i]\n",
    "            if certainty_metric == \"max_prob\":\n",
    "                shin_conf = s_probs.max().item()\n",
    "            elif certainty_metric == \"entropy\":\n",
    "                entropy = -sum(p.item() * math.log2(p.item()) for p in s_probs if p.item() > 0)\n",
    "                shin_conf = 1 - entropy / math.log2(len(s_probs))\n",
    "        else:\n",
    "            shin_conf = None\n",
    "\n",
    "        # ---- combine uncertainties ----\n",
    "        if shin_conf is not None:\n",
    "            if combine == \"max\":\n",
    "                conf = min(nikud_conf, shin_conf)  # lower = less certain\n",
    "            elif combine == \"avg\":\n",
    "                conf = (nikud_conf + shin_conf) / 2\n",
    "        else:\n",
    "            conf = nikud_conf\n",
    "\n",
    "        # Normalize into [0,1]\n",
    "        norm = (conf - low_conf) / (high_conf - low_conf)\n",
    "        norm = min(max(norm, 0.0), 1.0)\n",
    "\n",
    "        # Scaling\n",
    "        if scale == \"sqrt\":\n",
    "            norm = norm**0.5\n",
    "        elif scale == \"log\":\n",
    "            norm = (math.log1p(norm * 9) / math.log1p(9)) if norm > 0 else 0\n",
    "\n",
    "        # Bright colormap: yellow → orange → red\n",
    "        cmap = matplotlib.cm.get_cmap(\"YlOrRd\")\n",
    "        rgba = cmap(1 - norm)  # invert so red = uncertain\n",
    "        color = matplotlib.colors.rgb2hex(rgba)\n",
    "\n",
    "        html_chars.append(f\"<span style='color:{color}'>{char}</span>\")\n",
    "\n",
    "    display(HTML(\"\".join(html_chars)))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "colorize_text_by_certainty(\"דה הוושי, שהיה כימאי פיזיקלי הונגרי ממוצא יהודי,\", model, tokenizer,\n",
    "                           certainty_metric=\"max_prob\",\n",
    "                           scale=\"sqrt\", low_conf=0.6, high_conf=0.95)\n",
    "colorize_text_by_certainty(\"שמחה האח חבר טוב של כדרלעומר\", model, tokenizer,\n",
    "                           certainty_metric=\"max_prob\",\n",
    "                           scale=\"sqrt\", low_conf=0.6, high_conf=0.95)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikud-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
